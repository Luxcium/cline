[**claude-dev**](../../../README.md)

***

[claude-dev](../../../README.md) / [shared/api](../README.md) / groqModels

# Variable: groqModels

> `const` **groqModels**: `object`

Defined in: src/shared/api.ts:2708

## Type declaration

### compound-beta

> `readonly` **compound-beta**: `object`

#### compound-beta.contextWindow

> `readonly` **contextWindow**: `128000` = `128000`

#### compound-beta.description

> `readonly` **description**: `"Compound model using Llama 4 Scout for core reasoning with Llama 3.3 70B for routing and tool use. Excellent for plan/act workflows."` = `"Compound model using Llama 4 Scout for core reasoning with Llama 3.3 70B for routing and tool use. Excellent for plan/act workflows."`

#### compound-beta.inputPrice

> `readonly` **inputPrice**: `0` = `0.0`

#### compound-beta.maxTokens

> `readonly` **maxTokens**: `8192` = `8192`

#### compound-beta.outputPrice

> `readonly` **outputPrice**: `0` = `0.0`

#### compound-beta.supportsImages

> `readonly` **supportsImages**: `false` = `false`

#### compound-beta.supportsPromptCache

> `readonly` **supportsPromptCache**: `false` = `false`

### compound-beta-mini

> `readonly` **compound-beta-mini**: `object`

#### compound-beta-mini.contextWindow

> `readonly` **contextWindow**: `128000` = `128000`

#### compound-beta-mini.description

> `readonly` **description**: `"Lightweight compound model for faster inference while maintaining tool use capabilities."` = `"Lightweight compound model for faster inference while maintaining tool use capabilities."`

#### compound-beta-mini.inputPrice

> `readonly` **inputPrice**: `0` = `0.0`

#### compound-beta-mini.maxTokens

> `readonly` **maxTokens**: `8192` = `8192`

#### compound-beta-mini.outputPrice

> `readonly` **outputPrice**: `0` = `0.0`

#### compound-beta-mini.supportsImages

> `readonly` **supportsImages**: `false` = `false`

#### compound-beta-mini.supportsPromptCache

> `readonly` **supportsPromptCache**: `false` = `false`

### deepseek-r1-distill-llama-70b

> `readonly` **deepseek-r1-distill-llama-70b**: `object`

#### deepseek-r1-distill-llama-70b.contextWindow

> `readonly` **contextWindow**: `131072` = `131072`

#### deepseek-r1-distill-llama-70b.description

> `readonly` **description**: `"DeepSeek R1 reasoning capabilities distilled into Llama 70B architecture. Excellent for complex problem-solving and planning."` = `"DeepSeek R1 reasoning capabilities distilled into Llama 70B architecture. Excellent for complex problem-solving and planning."`

#### deepseek-r1-distill-llama-70b.inputPrice

> `readonly` **inputPrice**: `0.75` = `0.75`

#### deepseek-r1-distill-llama-70b.maxTokens

> `readonly` **maxTokens**: `131072` = `131072`

#### deepseek-r1-distill-llama-70b.outputPrice

> `readonly` **outputPrice**: `0.99` = `0.99`

#### deepseek-r1-distill-llama-70b.supportsImages

> `readonly` **supportsImages**: `false` = `false`

#### deepseek-r1-distill-llama-70b.supportsPromptCache

> `readonly` **supportsPromptCache**: `false` = `false`

#### llama-3.1-8b-instant

> `readonly` **llama-3.1-8b-instant**: `object`

#### llama-3.1-8b-instant.contextWindow

> `readonly` **contextWindow**: `131072` = `131072`

#### llama-3.1-8b-instant.description

> `readonly` **description**: `"Fast and efficient Llama 3.1 8B model optimized for speed, low latency, and reliable tool execution."` = `"Fast and efficient Llama 3.1 8B model optimized for speed, low latency, and reliable tool execution."`

#### llama-3.1-8b-instant.inputPrice

> `readonly` **inputPrice**: `0.05` = `0.05`

#### llama-3.1-8b-instant.maxTokens

> `readonly` **maxTokens**: `131072` = `131072`

#### llama-3.1-8b-instant.outputPrice

> `readonly` **outputPrice**: `0.08` = `0.08`

#### llama-3.1-8b-instant.supportsImages

> `readonly` **supportsImages**: `false` = `false`

#### llama-3.1-8b-instant.supportsPromptCache

> `readonly` **supportsPromptCache**: `false` = `false`

#### llama-3.3-70b-versatile

> `readonly` **llama-3.3-70b-versatile**: `object`

#### llama-3.3-70b-versatile.contextWindow

> `readonly` **contextWindow**: `131072` = `131072`

#### llama-3.3-70b-versatile.description

> `readonly` **description**: `"Meta's latest Llama 3.3 70B model optimized for versatile use cases with excellent performance and speed."` = `"Meta's latest Llama 3.3 70B model optimized for versatile use cases with excellent performance and speed."`

#### llama-3.3-70b-versatile.inputPrice

> `readonly` **inputPrice**: `0.59` = `0.59`

#### llama-3.3-70b-versatile.maxTokens

> `readonly` **maxTokens**: `32768` = `32768`

#### llama-3.3-70b-versatile.outputPrice

> `readonly` **outputPrice**: `0.79` = `0.79`

#### llama-3.3-70b-versatile.supportsImages

> `readonly` **supportsImages**: `false` = `false`

#### llama-3.3-70b-versatile.supportsPromptCache

> `readonly` **supportsPromptCache**: `false` = `false`

### meta-llama/llama-4-maverick-17b-128e-instruct

> `readonly` **meta-llama/llama-4-maverick-17b-128e-instruct**: `object`

#### meta-llama/llama-4-maverick-17b-128e-instruct.contextWindow

> `readonly` **contextWindow**: `131072` = `131072`

#### meta-llama/llama-4-maverick-17b-128e-instruct.description

> `readonly` **description**: `"Meta's Llama 4 Maverick 17B model with 128 experts, supports vision and multimodal tasks."` = `"Meta's Llama 4 Maverick 17B model with 128 experts, supports vision and multimodal tasks."`

#### meta-llama/llama-4-maverick-17b-128e-instruct.inputPrice

> `readonly` **inputPrice**: `0.2` = `0.2`

#### meta-llama/llama-4-maverick-17b-128e-instruct.maxTokens

> `readonly` **maxTokens**: `8192` = `8192`

#### meta-llama/llama-4-maverick-17b-128e-instruct.outputPrice

> `readonly` **outputPrice**: `0.6` = `0.6`

#### meta-llama/llama-4-maverick-17b-128e-instruct.supportsImages

> `readonly` **supportsImages**: `true` = `true`

#### meta-llama/llama-4-maverick-17b-128e-instruct.supportsPromptCache

> `readonly` **supportsPromptCache**: `false` = `false`

### meta-llama/llama-4-scout-17b-16e-instruct

> `readonly` **meta-llama/llama-4-scout-17b-16e-instruct**: `object`

#### meta-llama/llama-4-scout-17b-16e-instruct.contextWindow

> `readonly` **contextWindow**: `131072` = `131072`

#### meta-llama/llama-4-scout-17b-16e-instruct.description

> `readonly` **description**: `"Meta's Llama 4 Scout 17B model with 16 experts, optimized for fast inference and general tasks."` = `"Meta's Llama 4 Scout 17B model with 16 experts, optimized for fast inference and general tasks."`

#### meta-llama/llama-4-scout-17b-16e-instruct.inputPrice

> `readonly` **inputPrice**: `0.11` = `0.11`

#### meta-llama/llama-4-scout-17b-16e-instruct.maxTokens

> `readonly` **maxTokens**: `8192` = `8192`

#### meta-llama/llama-4-scout-17b-16e-instruct.outputPrice

> `readonly` **outputPrice**: `0.34` = `0.34`

#### meta-llama/llama-4-scout-17b-16e-instruct.supportsImages

> `readonly` **supportsImages**: `true` = `true`

#### meta-llama/llama-4-scout-17b-16e-instruct.supportsPromptCache

> `readonly` **supportsPromptCache**: `false` = `false`

### moonshotai/kimi-k2-instruct

> `readonly` **moonshotai/kimi-k2-instruct**: `object`

#### moonshotai/kimi-k2-instruct.contextWindow

> `readonly` **contextWindow**: `131072` = `131072`

#### moonshotai/kimi-k2-instruct.description

> `readonly` **description**: `"Kimi K2 is Moonshot AI's state-of-the-art Mixture-of-Experts (MoE) language model with 1 trillion total parameters and 32 billion activated parameters."` = `"Kimi K2 is Moonshot AI's state-of-the-art Mixture-of-Experts (MoE) language model with 1 trillion total parameters and 32 billion activated parameters."`

#### moonshotai/kimi-k2-instruct.inputPrice

> `readonly` **inputPrice**: `1` = `1.0`

#### moonshotai/kimi-k2-instruct.maxTokens

> `readonly` **maxTokens**: `16384` = `16384`

#### moonshotai/kimi-k2-instruct.outputPrice

> `readonly` **outputPrice**: `3` = `3.0`

#### moonshotai/kimi-k2-instruct.supportsImages

> `readonly` **supportsImages**: `false` = `false`

#### moonshotai/kimi-k2-instruct.supportsPromptCache

> `readonly` **supportsPromptCache**: `false` = `false`

### openai/gpt-oss-120b

> `readonly` **openai/gpt-oss-120b**: `object`

#### openai/gpt-oss-120b.contextWindow

> `readonly` **contextWindow**: `131072` = `131_072`

#### openai/gpt-oss-120b.description

> `readonly` **description**: `"A state-of-the-art 120B open-weight Mixture-of-Experts language model optimized for strong reasoning, tool use, and efficient deployment on large GPUs"` = `"A state-of-the-art 120B open-weight Mixture-of-Experts language model optimized for strong reasoning, tool use, and efficient deployment on large GPUs"`

#### openai/gpt-oss-120b.inputPrice

> `readonly` **inputPrice**: `0.15` = `0.15`

#### openai/gpt-oss-120b.maxTokens

> `readonly` **maxTokens**: `32766` = `32766`

#### openai/gpt-oss-120b.outputPrice

> `readonly` **outputPrice**: `0.75` = `0.75`

#### openai/gpt-oss-120b.supportsImages

> `readonly` **supportsImages**: `false` = `false`

#### openai/gpt-oss-120b.supportsPromptCache

> `readonly` **supportsPromptCache**: `false` = `false`

### openai/gpt-oss-20b

> `readonly` **openai/gpt-oss-20b**: `object`

#### openai/gpt-oss-20b.contextWindow

> `readonly` **contextWindow**: `131072` = `131_072`

#### openai/gpt-oss-20b.description

> `readonly` **description**: `"A compact 20B open-weight Mixture-of-Experts language model designed for strong reasoning and tool use, ideal for edge devices and local inference."` = `"A compact 20B open-weight Mixture-of-Experts language model designed for strong reasoning and tool use, ideal for edge devices and local inference."`

#### openai/gpt-oss-20b.inputPrice

> `readonly` **inputPrice**: `0.1` = `0.1`

#### openai/gpt-oss-20b.maxTokens

> `readonly` **maxTokens**: `32766` = `32766`

#### openai/gpt-oss-20b.outputPrice

> `readonly` **outputPrice**: `0.5` = `0.5`

#### openai/gpt-oss-20b.supportsImages

> `readonly` **supportsImages**: `false` = `false`

#### openai/gpt-oss-20b.supportsPromptCache

> `readonly` **supportsPromptCache**: `false` = `false`
